# -*- coding: utf-8 -*-
"""design_forecast_lstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GL3t11uunq_7sjJbdrNrT3OKQ-BDgaYC

# ç”¨æ°´é‡é æ¸¬ï¼ˆå€åŸŸ + è¾¦å…¬å®¤ï¼‰LSTM æ¨¡å‹
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è¨­å‚™: {device}")

class EnergyDataset(Dataset):
    def __init__(self, values, months, seq_length):
        self.values = values
        self.months = months
        self.seq_length = seq_length

    def __len__(self):
        return len(self.values) - self.seq_length

    def __getitem__(self, idx):
        x_val = self.values[idx:idx+self.seq_length]             # shape: (seq,)
        x_mon = self.months[idx:idx+self.seq_length]             # shape: (seq, 2)

        # ä¿è­‰å½¢ç‹€æ­£ç¢º
        x_val = x_val.view(-1, 1)                                # shape: (seq, 1)
        x = torch.cat([x_val, x_mon], dim=1)                     # shape: (seq, 3)

        y = self.values[idx + self.seq_length].unsqueeze(-1)     # shape: (1,)
        return x, y


class EnergyPredictorLSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):
        super(EnergyPredictorLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        if x.dim() == 4:
            x = x.squeeze(-1)
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return out

def train_lstm(train_data, month_array, seq_length=12, epochs=100, show_loss_curve=True, model_name="predict_lstm"):
    # Normalize water usage
    scaler = MinMaxScaler()
    values_scaled = scaler.fit_transform(train_data.reshape(-1, 1))
    values_tensor = torch.tensor(values_scaled, dtype=torch.float32)

    # Encode month as sin/cos
    month_array = np.array(month_array)
    month_sin = np.sin(2 * np.pi * month_array / 12)
    month_cos = np.cos(2 * np.pi * month_array / 12)
    month_features = torch.tensor(np.stack([month_sin, month_cos], axis=1), dtype=torch.float32)

    # Dataset + Model
    dataset = EnergyDataset(values_tensor, month_features, seq_length)
    loader = DataLoader(dataset, batch_size=4, shuffle=True)

    # âœ… ä½¿ç”¨æ›´å¼·å¤§çš„æ¨¡å‹åƒæ•¸
    model = EnergyPredictorLSTM(input_size=3, hidden_size=256, num_layers=4, dropout=0.3).to(device)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    epoch_losses = []

    for epoch in tqdm(range(epochs), desc=f"Training {model_name or ''}"):
        running_loss = 0.0
        for x_batch, y_batch in loader:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            output = model(x_batch)
            loss = criterion(output, y_batch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        epoch_losses.append(running_loss / len(loader))

    if show_loss_curve:
        plt.figure(figsize=(8, 4))
        plt.plot(range(1, epochs + 1), epoch_losses, marker='o')
        plt.title("Training Loss Over Epochs")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.grid(True)
        plt.tight_layout()
        plt.show()

    if model_name:
        torch.save(model.state_dict(), f"{model_name}.pt")
        print(f"[âœ“] Model saved as {model_name}.pt")

    return model, scaler

def multi_step_forecast(model, scaler, history_data, history_months, steps=2, seq_length=12):
    model.eval()

    # Prepare input
    values_scaled = scaler.transform(history_data[-seq_length:].reshape(-1, 1))
    month_array = np.array(history_months[-seq_length:])
    month_sin = np.sin(2 * np.pi * month_array / 12)
    month_cos = np.cos(2 * np.pi * month_array / 12)
    month_features = np.stack([month_sin, month_cos], axis=1)

    seq_input = torch.tensor(np.concatenate([values_scaled, month_features], axis=1), dtype=torch.float32).unsqueeze(0).to(device)

    predictions = []
    next_month = history_months[-1]
    with torch.no_grad():
        for _ in range(steps):
            pred = model(seq_input).item()
            pred_orig = scaler.inverse_transform(np.array([[pred]]))[0, 0]
            predictions.append(pred_orig)

            next_month = 1 if next_month == 12 else next_month + 1
            sin_next = np.sin(2 * np.pi * next_month / 12)
            cos_next = np.cos(2 * np.pi * next_month / 12)
            new_entry = torch.tensor([[[pred, sin_next, cos_next]]], dtype=torch.float32).to(device)
            seq_input = torch.cat((seq_input[:, 1:, :], new_entry), dim=1)

    return predictions

def process_series_and_forecast(series, months, forecast_steps=12):
    model, scaler = train_lstm(series, months, model_name="predict_lstm")
    predictions = multi_step_forecast(model, scaler, series, months, steps=forecast_steps)
    return predictions

water_file_path = "./train_data/Monthly_Water_2021-2023.xlsx"
electricity_file_path = "./train_data/Monthly_electricity_2021-2023.xlsx"
df_water = pd.read_excel(water_file_path, sheet_name="Sheet1")
df_electricity = pd.read_excel(electricity_file_path, sheet_name="Sheet1")

water_columns = ['Total_Water(ML)']
electricity_columns = ['Total Electricity Consumption (kWh)']
forecast_steps = 3  # é æ¸¬æœªä¾† 3 å€‹æœˆ

# é æ¸¬å„å€åŸŸ
water_forecasts = {}
for col in water_columns:
    series = df_water[col].dropna().values
    valid_idx = df_water[col].dropna().index
    months = df_water.loc[valid_idx, 'Month'].values
    water_forecasts[col] = process_series_and_forecast(series, months, forecast_steps=forecast_steps)

# é æ¸¬å„è¾¦å…¬å®¤
electricity_forecasts = {}
for col in electricity_columns:
    series = df_electricity[col].dropna().values
    valid_idx = df_electricity[col].dropna().index
    months = df_electricity.loc[valid_idx, 'Month'].values
    electricity_forecasts[col] = process_series_and_forecast(series, months, forecast_steps=forecast_steps)

def plot_forecasts(history_df, forecasts, title, date_df=None, forecast_steps=12):
    plt.figure(figsize=(14, 6))
    color_cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']
    color_map = {}
    color_index = 0

    for col in forecasts:
        full_series = history_df[col].dropna().values
        history_tail = full_series[-5:]
        prediction = forecasts[col]

        # çµ±ä¸€é¡è‰²
        if col not in color_map:
            color_map[col] = color_cycle[color_index % len(color_cycle)]
            color_index += 1
        color = color_map[col]

        # X è»¸æ¨™ç±¤è™•ç†
        if date_df is not None:
            # å¼·åŒ–é˜²å‘†ï¼šç¼ºå¤±å€¼å¡«å…¥é è¨­å€¼
            months_str = date_df['Month'].fillna(1).astype(int).astype(str).str.zfill(2)
            years_str = date_df['Year'].fillna(2000).astype(int).astype(str)
            dates = pd.to_datetime(years_str + '-' + months_str, format="%Y-%m")

            label_dates = dates.iloc[-5:].tolist()
            last_date = label_dates[-1]
            for i in range(1, forecast_steps + 1):
                next_month = last_date + pd.DateOffset(months=i)
                label_dates.append(next_month)
            x_labels = [d.strftime("%Y-%m") for d in label_dates]
        else:
            x_labels = [f"T-{5 - i}" for i in range(5)] + [f"F+{i+1}" for i in range(forecast_steps)]

        # ç•«ç·š
        plt.plot(range(5), history_tail, label=f"{col} - History", color=color)
        plt.plot(range(5, 5 + forecast_steps), prediction, 'x--', label=f"{col} - Forecast", color=color)

        # æ•¸å€¼æ¨™è¨»
        for i, v in enumerate(history_tail):
            plt.text(i, v, f"{v:.1f}", ha='center', fontsize=10)
        for i, v in zip(range(5, 5 + forecast_steps), prediction):
            plt.text(i, v, f"{v:.1f}", ha='center', fontsize=10, color='red')

    plt.title(title)
    plt.xlabel("Month")
    plt.ylabel("Water Usage")
    plt.xticks(ticks=range(5 + forecast_steps), labels=x_labels, rotation=45)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# ç•«åœ–
plot_forecasts(df_water[water_columns], water_forecasts, "Water Usage Forecast (Next 3 Months)", date_df=df_water[['Year', 'Month']], forecast_steps=forecast_steps)
plot_forecasts(df_electricity[electricity_columns], electricity_forecasts, "Electricity Usage Forecast (Next 3 Months)", date_df=df_electricity[['Year', 'Month']], forecast_steps=forecast_steps)

# =========  æ–°å¢å€å¡Šï¼šè¼‰å…¥å·²å„²å­˜æ¨¡å‹ä¸¦é æ¸¬ä¸‹ä¸€å€‹æœˆ  =========

def load_trained_model(model_path):
    """
    å»ºç«‹èˆ‡è¨“ç·´æ™‚ç›¸åŒæ¶æ§‹çš„ LSTMï¼Œä¸¦è¼‰å…¥æ¬Šé‡ã€‚
    """
    model = EnergyPredictorLSTM(input_size=3, hidden_size=256,
                                num_layers=4, dropout=0.3).to(device)
    state = torch.load(model_path, map_location=device)
    model.load_state_dict(state)
    model.eval()
    return model

def predict_next_month(series, months, model_path):
    """
    ä½¿ç”¨æŒ‡å®šçš„ .pt æª”ï¼Œæ ¹æ“šæ­·å² series / months é æ¸¬ä¸‹ä¸€å€‹æœˆã€‚
    å›å‚³å–®ä¸€ floatã€‚
    """
    # 1ï¸âƒ£ é‡æ–°æ“¬åˆ scalerï¼ˆç”¨å®Œæ•´æ­·å²è³‡æ–™ï¼‰
    scaler = MinMaxScaler()
    scaler.fit(series.reshape(-1, 1))

    # 2ï¸âƒ£ è¼‰å…¥æ¨¡å‹
    model = load_trained_model(model_path)

    # 3ï¸âƒ£ multi-step é æ¸¬ï¼ˆåªå– steps=1ï¼‰
    next_val = multi_step_forecast(model, scaler,
                                   history_data=series,
                                   history_months=months,
                                   steps=1)[0]
    return next_val

# ----------  è·¯å¾‘ & æ¬„ä½è¨­å®š ----------
water_model_path = "predict_lstm.pt"          # â† ä¾å¯¦éš›æª”åä¿®æ”¹
elec_model_path  = "predict_lstm.pt"           # â† ä¾å¯¦éš›æª”åä¿®æ”¹

water_col = 'Total_Water(ML)'
elec_col  = 'Total Electricity Consumption (kWh)'

# ----------  æº–å‚™è³‡æ–™ ----------
series_water = df_water[water_col].dropna().values
months_water = df_water.loc[df_water[water_col].dropna().index, 'Month'].values

series_elec = df_electricity[elec_col].dropna().values
months_elec = df_electricity.loc[df_electricity[elec_col].dropna().index, 'Month'].values

# ----------  é æ¸¬ ----------
next_water = predict_next_month(series_water, months_water, water_model_path)
next_elec  = predict_next_month(series_elec, months_elec, elec_model_path)

# ----------  è¼¸å‡º ----------
print(f"ğŸ”¹ æœ¬æœˆç”¨æ°´é‡ï¼š{series_water[-1]:,.2f} ML")
print(f"ğŸ”¹ æœ¬æœˆç”¨é›»é‡ï¼š{series_elec[-1]:,.2f} kWh")
print(f"ğŸ”¹ ä¸‹å€‹æœˆé ä¼°ç”¨æ°´é‡ï¼š{next_water:,.2f} ML")
print(f"ğŸ”¹ ä¸‹å€‹æœˆé ä¼°ç”¨é›»é‡ï¼š{next_elec:,.2f} kWh")

# =========  LLMè¼‰å…¥ =========
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name="Qwen/Qwen2-1.5B-Chat"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
     model_name,
     torch_dtype=torch.float16,
     device_map="auto"
     )

def generate_advice(prompt, tokenizer=tokenizer, model=model, max_new_tokens=160):
    """
    ä½¿ç”¨å°å‹ LLM ç”¢ç”Ÿç¹é«”ä¸­æ–‡å»ºè­°ã€‚
    """

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9
        )
    advice = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return advice.strip()

# ----------  è¨ˆç®—è¶¨å‹¢ ----------
curr_water = series_water[-1]
curr_elec  = series_elec[-1]

water_diff = next_water - curr_water
elec_diff  = next_elec  - curr_elec

water_pct = water_diff / curr_water * 100
elec_pct  = elec_diff  / curr_elec  * 100

trend_water = "ä¸Šå‡" if water_diff > 1e-3 else "ä¸‹é™" if water_diff < -1e-3 else "æŒå¹³"
trend_elec  = "ä¸Šå‡" if elec_diff  > 1e-3 else "ä¸‹é™" if elec_diff  < -1e-3 else "æŒå¹³"

# ----------  è¼¸å‡º ----------
print("============ ç”¨æ°´ / ç”¨é›» é æ¸¬æ¦‚è¦½ ============")
print(f"ğŸ”¹ æœ¬æœˆç”¨æ°´é‡ï¼š{curr_water:,.2f} ML")
print(f"ğŸ”¹ ä¸‹æœˆé ä¼°ç”¨æ°´é‡ï¼š{next_water:,.2f} MLï¼ˆ{trend_water} {water_pct:+.1f}%ï¼‰")
print(f"ğŸ”¹ æœ¬æœˆç”¨é›»é‡ï¼š{curr_elec:,.2f} kWh")
print(f"ğŸ”¹ ä¸‹æœˆé ä¼°ç”¨é›»é‡ï¼š{next_elec:,.2f} kWhï¼ˆ{trend_elec} {elec_pct:+.1f}%ï¼‰")

# ----------  LLM å»ºè­° ----------
prompt = (
    f"ä½ æ˜¯ä¸€ä½ç¯€èƒ½èˆ‡ç¯€æ°´é¡§å•ï¼Œè«‹ä¾æ“šä»¥ä¸‹æ•¸æ“šæ¢åˆ—3é»ç¹é«”ä¸­æ–‡å»ºè­°ã€‚\n"
    f"- æœ¬æœˆç”¨æ°´ {curr_water:.2f} MLï¼Œé ä¼°ä¸‹æœˆ {next_water:.2f} MLï¼Œ{trend_water} {water_pct:+.1f}%\n"
    f"- æœ¬æœˆç”¨é›» {curr_elec:.2f} kWhï¼Œé ä¼°ä¸‹æœˆ {next_elec:.2f} kWhï¼Œ{trend_elec} {elec_pct:+.1f}%\n"
    f"å¯ä»¥å¾ä¸€äº›æ—¥å¸¸ç¿’æ…£èˆ‡å¸¸è¦‹çš„é›»å™¨ä½¿ç”¨æ–¹å¼ä¾†å»ºè­°ã€‚"
)

advice = generate_advice(prompt)
print("\n============ å»ºè­° ============")
print(advice)
print("================================")

# =========  å¯«å…¥ JSON =========
import json
from datetime import datetime

# å°‡è¦å„²å­˜çš„æ¬„ä½æ•´ç†æˆ dict
result_dict = {
    "æœ¬æœˆç”¨æ°´é‡": f"{curr_water:,.2f} ML",
    "ä¸‹æœˆé ä¼°ç”¨æ°´é‡": f"{next_water:,.2f} MLï¼ˆ{trend_water} {water_pct:+.1f}%ï¼‰",
    "æœ¬æœˆç”¨é›»é‡": f"{curr_elec:,.2f} kWh",
    "ä¸‹æœˆé ä¼°ç”¨é›»é‡": f"{next_elec:,.2f} kWhï¼ˆ{trend_elec} {elec_pct:+.1f}%ï¼‰",
    "LLMå»ºè­°": advice
}

output_path = "predict_summary.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(result_dict, f, ensure_ascii=False, indent=2)

print(f"âœ… å·²å°‡çµæœå­˜æˆ JSONï¼š{output_path}")